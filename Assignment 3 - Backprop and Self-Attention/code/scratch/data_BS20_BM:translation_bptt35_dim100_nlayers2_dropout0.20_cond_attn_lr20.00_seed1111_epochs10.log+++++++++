main.py --train --cond --batch_method translation --attn

Building data from ./data...
      batch_size: 20
batch_size_valid: 60
    batch_method: translation      (no sorting by target lengths)
          device: cpu
  is_conditional: True

15 batches
5 batches

vocab_size: 1282

train.txt
              # words: 6681
               # seqs: 300
  avg/max/min lengths: 22/72/3

src-train.txt
              # words: 6081
               # seqs: 300
  avg/max/min lengths: 20/70/1

Seq2Seq
      # parameters: 522682
        vocab_size: 1282
               dim: 100
          # layers: 2
    is_conditional: 1
     bidirectional: 0
        use_bridge: 0
     use_attention: 1

Control
            lr: 20.00
          bptt: 35

| epoch   1 |    20/   29 batches | lr 20.00 | ms/batch 292.80 | loss  7.62 | ppl  2037.78
-----------------------------------------------------------------------------------------
| end of epoch   1 | time:  8.90s | valid loss  5.94 | valid ppl   380.12 | valid sqxent   126.35
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
Exiting from training early
=========================================================================================
| End of training | final loss  5.94 | final ppl   380.12 | final sqxent   126.35
=========================================================================================
00:00:15
