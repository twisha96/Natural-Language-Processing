main.py --train --cond --batch_method translation

Building data from ./data...
      batch_size: 20
batch_size_valid: 60
    batch_method: translation      (no sorting by target lengths)
          device: cpu
  is_conditional: True

15 batches
5 batches

vocab_size: 1282

train.txt
              # words: 6681
               # seqs: 300
  avg/max/min lengths: 22/72/3

src-train.txt
              # words: 6081
               # seqs: 300
  avg/max/min lengths: 20/70/1

Seq2Seq
      # parameters: 452682
        vocab_size: 1282
               dim: 100
          # layers: 2
    is_conditional: 1
     bidirectional: 0
        use_bridge: 0
     use_attention: 0

Control
            lr: 20.00
          bptt: 35

| epoch   1 |    20/   29 batches | lr 20.00 | ms/batch 70.59 | loss  6.18 | ppl   485.31
-----------------------------------------------------------------------------------------
| end of epoch   1 | time:  2.47s | valid loss  5.72 | valid ppl   304.54 | valid sqxent   121.64
-----------------------------------------------------------------------------------------
| epoch   2 |    20/   29 batches | lr 20.00 | ms/batch 67.92 | loss  4.99 | ppl   147.51
-----------------------------------------------------------------------------------------
Exiting from training early
=========================================================================================
| End of training | final loss  5.72 | final ppl   304.54 | final sqxent   121.64
=========================================================================================
00:00:05
